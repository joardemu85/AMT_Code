{"cells":[{"cell_type":"markdown","metadata":{"id":"2wwwjhfPIeNY"},"source":["Written by Jorge: 20240429\n","\n","Copied from 1g_EEGNET..\n","\n","On this notebook I will try to train modelsf based on DeepConVNet, also available from the EEGNet library\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SGf9VOgEN_fV"},"source":["# **1. Instancies and libraries**"]},{"cell_type":"markdown","metadata":{"id":"iqRfwbDuJ_bA"},"source":["## 1.1 Install MNE to visualize and, potentially, process EEG data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SStyYYs8p6Fm"},"outputs":[],"source":["pip install mne"]},{"cell_type":"markdown","metadata":{"id":"8YDVqpYDKlDo"},"source":["## 1.2 Add EEGNet library to the environmental variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HZXtIJGgJj8X"},"outputs":[],"source":["import sys\n","path = \"/content/EEGNET\"\n","sys.path.append(path)"]},{"cell_type":"markdown","metadata":{"id":"f44oLjpwIwuU"},"source":["## 1.3 **Import** the main libraries. This ones will be used across all the program"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YIskn6nJXUiK"},"outputs":[],"source":["# Filesystem\n","import os\n","import zipfile\n","\n","# data processing\n","import numpy as np\n","import pandas as pd\n","\n","# AI-related\n","import tensorflow as tf\n","from tensorflow.keras.callbacks import Callback\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.metrics import confusion_matrix\n","import itertools\n","\n","# Plotting\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","# MNE\n","import mne\n","from mne import io\n","\n","#EEGNET\n","from EEGModels import ShallowConvNet\n","from tensorflow.keras import utils as np_utils\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras import backend as K"]},{"cell_type":"markdown","metadata":{"id":"rM-UzTbfjzCy"},"source":["# **2. Functions**"]},{"cell_type":"markdown","metadata":{"id":"0MkbqCZwNw4A"},"source":["## 2.1 Function to visualize the performance of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sD8OX4ZbdGFz"},"outputs":[],"source":["def visualize_results (model, n_epochs):\n","  epochs = [i for i in range (n_epochs)]\n","  fig, ax = plt.subplots(1,2)\n","  train_acc = model.history[\"accuracy\"]\n","  train_loss = model.history[\"loss\"]\n","  val_acc = model.history[\"val_accuracy\"]\n","  val_loss = model.history[\"val_loss\"]\n","  fig.set_size_inches(16,9)\n","\n","  ax[0].plot(epochs, train_acc, \"go-\", label = \"Training Accuracy\")\n","  ax[0].plot(epochs, val_acc, \"ro-\", label = \"Validation Accuracy\")\n","  ax[0].set_title(\"Training and Validation Accuracy\")\n","  ax[0].legend()\n","  ax[0].set_xlabel(\"Epochs\")\n","  ax[0].set_ylabel(\"Accuracy\")\n","\n","  ax[1].plot(epochs, train_loss, \"go-\", label = \"Training Loss\")\n","  ax[1].plot(epochs, val_loss, \"ro-\", label = \"Validation Loss\")\n","  ax[1].set_title(\"Training and Validation Loss\")\n","  ax[1].legend()\n","  ax[1].set_xlabel(\"Epochs\")\n","  ax[1].set_ylabel(\"Loss\")\n","\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"bio4y2lON8WA"},"source":["## 2.2 **Function** to plot Confusion Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAZ6UoZJapBK"},"outputs":[],"source":["def plot_confusion_matrix (cm,\n","                           classes,\n","                           normalize = False,\n","                           title = 'Confusion Matrix',\n","                           cmap=plt.cm.Greens\n","                          ):\n","  plt.imshow (cm, interpolation='nearest', cmap=cmap)\n","  plt.title (title)\n","  plt.colorbar()\n","  tick_marks = np.arange(len(classes))\n","  plt.xticks(tick_marks, classes, rotation=45)\n","  plt.yticks(tick_marks, classes)\n","\n","  if normalize:\n","    cm=cm.astype('float')/cm.sum(axis=1)[:, np.newaxis]\n","    print(\"Normalized confusion matrix\")\n","  else:\n","    print(\"Confusion matrix, without normalization\")\n","  print(cm)\n","\n","  thresh = cm.max()*0.80\n","  for i, j in itertools.product (range(cm.shape[0]), range(cm.shape[1])):\n","     plt.text(j, i, round(cm[i,j],2),\n","              horizontalalignment=\"center\",\n","              color=\"white\" if cm [i, j] > thresh else \"black\")\n","\n","  plt.tight_layout()\n","  plt.ylabel('True Label')\n","  plt.xlabel('Predicted Label')"]},{"cell_type":"markdown","metadata":{"id":"mH0BeEq5PGGG"},"source":["## 2.3 Function to normalize EEG data (run inside the loop to assemble the tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GEW1rJ2YZPaH"},"outputs":[],"source":["def normalize_channels (data, n_channels):\n","  data_norm = np.zeros_like(data, dtype='float32')\n","  for ch in range (n_channels):\n","      min_val = (np.min(data[ch]))\n","      max_val = (np.max(data[ch]))\n","      data_norm [ch] = (data[ch] - min_val) / (max_val - min_val)\n","\n","  return data_norm"]},{"cell_type":"markdown","metadata":{"id":"9w782Xw3llmp"},"source":["# **3. Data Load**"]},{"cell_type":"markdown","metadata":{"id":"en_-72ntl40p"},"source":["## 3.1 Unzip dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KCwm5FJFsnrc"},"outputs":[],"source":["local_zip = \"/content/ERP_Pretest_Data_128Hz.zip\"\n","zip_ref = zipfile.ZipFile(local_zip, \"r\")\n","zip_ref.extractall(\"/content/ERP_Pretest_Data_128_Hz\")\n","zip_ref.close()"]},{"cell_type":"markdown","metadata":{"id":"Zp6AMQBzmE7_"},"source":["## 3.2 Define file paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7PW8miKzNft8"},"outputs":[],"source":["train_data_path = \"/content/ERP_Pretest_Data_128_Hz/Train/\"\n","test_data_path = \"/content/ERP_Pretest_Data_128_Hz/Test/\""]},{"cell_type":"markdown","metadata":{"id":"kjnq9azEmhy5"},"source":["## 3.3 Load file lists"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2fcwKxwRmg3M"},"outputs":[],"source":["trainlist = open  (\"/content/ERP_Pretest_Data_128_Hz/TrainFileList.txt\", \"r\")\n","data = trainlist.read()\n","train_data_all_files = data.split(\"\\n\")\n","\n","testlist = open  (\"/content/ERP_Pretest_Data_128_Hz/TestFileList.txt\", \"r\")\n","data_ = testlist.read()\n","test_data_all_files = data_.split(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"V7AHe_dBnFdi"},"source":["## 3.4 Determine the number of trials available based on the number of files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ntECh0BnF-P"},"outputs":[],"source":["n_trials_train = len(train_data_all_files)\n","n_trials_test = len(test_data_all_files)"]},{"cell_type":"markdown","metadata":{"id":"u0CPP8zBncMz"},"source":["## 3.5 Define the parameters for EEG data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eGfe4ryBni5Y"},"outputs":[],"source":["n_channels = 20\n","n_samples = 128\n","n_kernels = 1\n","\n","ch_names = ['Fz', 'F7', 'F3', 'F4', 'F8',\n","            'T7', 'C3', 'CZ', 'C4', 'T8',\n","            'P7', 'P3', 'Pz', 'P4', 'P8',\n","            'O1', 'Oz', 'O2', 'LM', 'RM']\n","\n","sfreq = 128\n","info = mne.create_info(ch_names = ch_names, sfreq = sfreq)"]},{"cell_type":"markdown","metadata":{"id":"xMOQ6dhEn2xE"},"source":["## 3.6 Load training data\n","\n","This data will be loaded as 2D array for augmentation and balancing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UnFZsXuAoJqk"},"outputs":[],"source":["train_set_2D = np.zeros ((n_trials_train,n_channels*n_samples), dtype='float32')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q21Z2NaQoPUP"},"outputs":[],"source":["trial = 0\n","for filename in train_data_all_files:\n","  data = pd.read_csv(filename, header=None, dtype=np.float32)\n","  trl = np.reshape (np.array(data), (n_channels*n_samples))\n","  train_set_2D[trial:] = trl\n","  trial=trial+1"]},{"cell_type":"markdown","metadata":{"id":"Bn0zyUjAoSyz"},"source":["Load labels for training data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJNpbYT4yStr"},"outputs":[],"source":["train_labels_path = '/content/ERP_Pretest_Data_128_Hz/TrainLabels.csv'\n","train_labels = np.array(pd.read_csv(train_labels_path, header=None, dtype='uint8'))\n","train_labels = np.squeeze(train_labels.T)"]},{"cell_type":"markdown","metadata":{"id":"QvMhBIOOoe1e"},"source":["Balance the training set using the Random Oversampler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GzM6zh1molgo"},"outputs":[],"source":["oversampler_train = RandomOverSampler(sampling_strategy='minority')\n","traindata_over, trainlabels_over = oversampler_train.fit_resample(train_set_2D, train_labels)\n","(overTrials_tr, overSamples_tr) = traindata_over.shape"]},{"cell_type":"markdown","metadata":{"id":"79kzgLkmo881"},"source":["Allocate data in tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G53bYJkgo8AI"},"outputs":[],"source":["X_train = np.zeros((overTrials_tr,n_channels,n_samples), dtype=np.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VqkNLOAkpHD-"},"outputs":[],"source":["for i in range(overTrials_tr):\n","  trl_rs = np.reshape (traindata_over[i],(n_channels,n_samples))\n","  # Optional operation to check if this improves the model performance\n","  trl_rs = normalize_channels(np.array(trl_rs, dtype='float32'), n_channels)\n","  X_train[i] = trl_rs\n","\n","# reasign the training labels\n","y_train = trainlabels_over\n","print(y_train.shape)"]},{"cell_type":"markdown","metadata":{"id":"Ec6SYegwqbLS"},"source":["Visualize the first trial in the set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A5xob1LuqhOJ"},"outputs":[],"source":["example = mne.io.RawArray(X_train[0], info)\n","example.plot(block=True)"]},{"cell_type":"markdown","metadata":{"id":"OKyh8Uobrofk"},"source":["## 3.7 Load test data\n","\n","This is a similar process to loading the training data, we perform oversampling here too"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CRk0KwG2r-lR"},"outputs":[],"source":["test_set_2D = np.zeros ((n_trials_test,n_channels*n_samples), dtype='float32')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7shweqKsGLf"},"outputs":[],"source":["trial = 0\n","for filename in test_data_all_files:\n","  data = pd.read_csv(filename, header=None, dtype=np.float32)\n","  trl = np.reshape (np.array(data), (n_channels*n_samples))\n","  test_set_2D[trial:] = trl\n","  trial=trial+1"]},{"cell_type":"markdown","metadata":{"id":"ckaADOhmyZhN"},"source":["Load test data labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ACFr4jqylvo"},"outputs":[],"source":["test_labels_path = '/content/ERP_Pretest_Data_128_Hz/TestLabels.csv'\n","test_labels = np.array(pd.read_csv(test_labels_path, header=None, dtype='uint8'))\n","test_labels = np.squeeze(test_labels.T)"]},{"cell_type":"markdown","metadata":{"id":"Eppq9TUA2NAj"},"source":["Balance with Random Oversampler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iy7JD-yhxyRe"},"outputs":[],"source":["oversampler_test = RandomOverSampler(sampling_strategy='minority')\n","testdata_over, testlabel_over = oversampler_test.fit_resample(test_set_2D, test_labels)\n","(overTrials_ts,overSamples_ts) = testdata_over.shape"]},{"cell_type":"markdown","metadata":{"id":"GcsyOhYqyd9J"},"source":["Allocate in tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C324P38g2S8s"},"outputs":[],"source":["X_test = np.zeros((overTrials_ts,n_channels,n_samples), dtype=np.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uevljSGf4A2H"},"outputs":[],"source":["for i in range(overTrials_ts):\n","  trl_rs = np.reshape (testdata_over[i],(n_channels,n_samples))\n","  # Optional operation to check if this improves the model performance\n","  trl_rs = normalize_channels(np.array(trl_rs, dtype='float32'), n_channels)\n","  X_test[i] = trl_rs\n","\n","# reasign the training labels\n","y_test = testlabel_over\n","print(y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"TpbIE7BB4yZY"},"source":["Visualize example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mOA-9-mp54St"},"outputs":[],"source":["test_example = mne.io.RawArray(X_test[0], info)\n","test_example.plot(block=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UxSu4IPF6DLn"},"outputs":[],"source":["print(X_train.shape)\n","print(X_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"FPqJgjHg6lVJ"},"source":["# **4. ShallowConvNet Portion**"]},{"cell_type":"markdown","metadata":{"id":"ZfN2llnGnD_3"},"source":["## 4.1 Define model architecture"]},{"cell_type":"markdown","metadata":{"id":"jnMrOaA7S6xQ"},"source":["Parameters for model training, let's start with the defaults"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6917LZiTCx2"},"outputs":[],"source":["# Model Build\n","classes = 2\n","dropout_rate = 0.2                  # hp.Float  ('dropoutRate',  min_value=0.2, max_value=0.5, sampling=\"log\")\n","# kernel_length = 64                  # hp.Choice ('kernLength', values = [16, 32, 64])\n","# f1 = 8                              # hp.Choice ('F1', values = [4, 8])\n","# d = 2                               # hp.Choice ('D', values = [1, 2])\n","# f2 = f1*d\n","# #dropout_type = 'SpatialDropout2D'   # hp.Choice ('dropoutType', values = ['Dropout', 'SpatialDropout'])\n","\n","# Compile\n","lr = 1e-4                           # hp.Float  ('learning_rate',  min_value=1e-6, max_value=1e-2, sampling=\"log\")\n","\n","batch = 16\n","epoch = 300"]},{"cell_type":"markdown","metadata":{"id":"Xb9Ez4wQnAPj"},"source":["Create the EEGNet model with the parameters above"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDWjgyqRnACU"},"outputs":[],"source":["ShallowConvNet_Model0 = ShallowConvNet(nb_classes = classes,\n","                       Chans = n_channels,\n","                       Samples = n_samples,\n","                       dropoutRate = dropout_rate)\n","\n","ShallowConvNet_Model0.summary()"]},{"cell_type":"markdown","source":["Let's save the architecture to use it in other models"],"metadata":{"id":"lxQmHYlL-URR"}},{"cell_type":"markdown","source":["\n"," ****"],"metadata":{"id":"19DRkzvIYaEC"}},{"cell_type":"markdown","source":["****UP TO THIS POINT EVERYTHING IS FIXED, FROM HERE ON WE START TO ITERATE****"],"metadata":{"id":"cqpbT1RhYM9R"}},{"cell_type":"markdown","source":["***\n"],"metadata":{"id":"uOm0C1YrYoFz"}},{"cell_type":"markdown","metadata":{"id":"YNqWDvkm7a4E"},"source":["## 4.2 Generate validation data\n","\n","First we generate an array containing all the possible indeces in the training set. From this, we are going to select 350 random values making sure they will not be repeated"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wF431PhU79zE"},"outputs":[],"source":["train_indeces = np.arange(0,overTrials_tr)\n","n_trials_val = overTrials_ts\n","val_indeces = np.random.choice(train_indeces, n_trials_val, replace=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYzgAhKP8BP1"},"outputs":[],"source":["val_indeces.sort()\n","print (val_indeces)"]},{"cell_type":"markdown","metadata":{"id":"EzUKhng9704-"},"source":["We create a container for the testing data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t_XGmTykuC33"},"outputs":[],"source":["X_val = np.zeros((n_trials_val,n_channels,n_samples), dtype=np.float32)\n","y_val =np.zeros (n_trials_val, dtype=np.uint8)\n","selected_trials = list(val_indeces)"]},{"cell_type":"markdown","metadata":{"id":"ehWHUDzl0ah4"},"source":["We start populating the validation subset the same way as before. This time we will use the data contained in training subset. Also, generate the labels for this subset in the same loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rxhCqvcVuLPZ"},"outputs":[],"source":["trial = 0\n","for i in selected_trials:\n","  X_val[trial::] = X_train[i]\n","  y_val[trial] = y_train[i]\n","  trial = trial+1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ndVC0sZ9HRy"},"outputs":[],"source":["print (X_val.shape)\n","print (y_val.shape)"]},{"cell_type":"markdown","metadata":{"id":"pFLSX2Wq9ivo"},"source":["Create a new training set by removing the selected trials for validation from the original"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lt9ll-lm2ZGA"},"outputs":[],"source":["X_train_new = np.delete (X_train, val_indeces, axis=0)\n","y_train_new = np.delete (y_train, val_indeces)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M_01Ls6o91k5"},"outputs":[],"source":["print (X_train_new.shape)\n","print (y_train_new.shape)"]},{"cell_type":"markdown","metadata":{"id":"mseP0Z6v-Emj"},"source":["## 4.3 Reshape data and labels to one-hot encodings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0OIJVomY-eC7"},"outputs":[],"source":["X_train_new = X_train_new.reshape(X_train_new.shape[0], n_channels, n_samples, n_kernels)\n","X_val_rs       = X_val.reshape(X_val.shape[0], n_channels, n_samples, n_kernels)\n","X_test_rs      = X_test.reshape(X_test.shape[0], n_channels, n_samples, n_kernels)\n","\n","print(X_train_new.shape)\n","print(X_val_rs.shape)\n","print(X_test_rs.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oFPhTv2BRX3Y"},"outputs":[],"source":["y_train_new = np_utils.to_categorical(y_train_new)\n","print(y_train_new.shape)\n","\n","y_val_rs = np_utils.to_categorical(y_val)\n","print (y_val_rs.shape)\n","\n","y_test_model = np_utils.to_categorical(y_test)\n","print (y_test_model.shape)"]},{"cell_type":"markdown","metadata":{"id":"Z7iHPipVteGM"},"source":["Training block"]},{"cell_type":"markdown","source":["Compile model"],"metadata":{"id":"Aqq_Cx7f2W9n"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oyvwOJntrJuH"},"outputs":[],"source":["import keras\n","from keras import optimizers\n","\n","opt = keras.optimizers.Adam(learning_rate = lr)\n","\n","ShallowConvNet_Model0.compile(loss='binary_crossentropy',\n","               optimizer=opt,\n","               metrics = ['accuracy'])\n"]},{"cell_type":"markdown","metadata":{"id":"AVu3GkQJr1r0"},"source":["Callback Block"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3fDyM8Jys8YY"},"outputs":[],"source":["#Checkpointer\n","checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint4.h5',\n","                               verbose=1,\n","                               save_best_only=True)"]},{"cell_type":"markdown","source":["Train model"],"metadata":{"id":"U8uNsDVA2biy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-N0Ff1DBtmMY"},"outputs":[],"source":["fittedModel1 = ShallowConvNet_Model0.fit(X_train_new,\n","                                 y_train_new,\n","                                 batch_size = batch,\n","                                 epochs = epoch,\n","                                 verbose = 2,\n","                                 validation_data=(X_val_rs, y_val_rs),\n","                                 callbacks=[checkpointer])\n","\n","# load optimal weights\n","ShallowConvNet_Model0.load_weights('/tmp/checkpoint4.h5')"]},{"cell_type":"markdown","metadata":{"id":"-4cJY65v3WUA"},"source":["Evaluate model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zcsy5rnCuWbn"},"outputs":[],"source":["y_probs       = ShallowConvNet_Model0.predict(X_test)\n","y_preds       = y_probs.argmax(axis = -1)\n","acc         = np.mean(y_preds == y_test_model.argmax(axis=-1))\n","print(\"Classification accuracy: %.2f \" % (acc))"]},{"cell_type":"markdown","metadata":{"id":"ZBBvGTOi3kVS"},"source":["Learning and loss plots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZK0PMAf13pog"},"outputs":[],"source":["visualize_results (fittedModel1, epoch)"]},{"cell_type":"markdown","metadata":{"id":"2R4eEnJ2345m"},"source":["Confusion Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X2stahOR37a6"},"outputs":[],"source":["cm = confusion_matrix((y_test), y_preds)\n","cm_plot_labels = ['Unknown', 'Known']\n","plot_confusion_matrix(cm, cm_plot_labels, normalize=False, title='Confusion Matrix')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DAVy-04WX4x8"},"outputs":[],"source":["cm = confusion_matrix((y_test), y_preds)\n","cm_plot_labels = ['Unknown', 'Known']\n","plot_confusion_matrix(cm, cm_plot_labels, normalize=True, title='Confusion Matrix')"]},{"cell_type":"markdown","source":["Save Model"],"metadata":{"id":"aaWs0sIMDXSw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VsOgHNQkkClX"},"outputs":[],"source":["!pip install pyyaml h5py"]},{"cell_type":"code","source":["ShallowConvNet_Model0.save('/content/Models/ShallowConvNet_Model0.h5')\n","ShallowConvNet_Model0.save('/content/Models/ShallowConvNet_Model0.keras')"],"metadata":{"id":"rkRHZZAxNGs5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Save predictions and test labels"],"metadata":{"id":"2wCJI2LeDZ5x"}},{"cell_type":"code","source":["import pandas as pd\n","outputfile = '/content/ShallowConvNet_Model.csv'\n","df = pd.DataFrame({'Y_Test': y_test, 'Unknown Prob': probs[:,0], 'Known_Prob': probs[:,1], 'Y_preds': preds})\n","df.to_csv(outputfile)\n"],"metadata":{"id":"K26epTksCu3l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["240516\n","\n","\n","Since it seems this model cannot be imported from the .h5 or .keras files, I can include something here to load new test data and perform more analysis."],"metadata":{"id":"PlY-f7gc377s"}}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}